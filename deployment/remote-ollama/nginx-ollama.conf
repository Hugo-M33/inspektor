# Nginx configuration for proxying Ollama API
# Add this to your nginx server block (e.g., /etc/nginx/sites-available/your-domain)

location /ollama/ {
    # Proxy to local Ollama service
    proxy_pass http://127.0.0.1:11434/;

    # Essential headers
    proxy_set_header Host $host;
    proxy_set_header X-Real-IP $remote_addr;
    proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
    proxy_set_header X-Forwarded-Proto $scheme;

    # WebSocket support for streaming responses
    proxy_http_version 1.1;
    proxy_set_header Upgrade $http_upgrade;
    proxy_set_header Connection "upgrade";

    # Timeouts for long-running LLM requests
    proxy_connect_timeout 300s;
    proxy_send_timeout 300s;
    proxy_read_timeout 300s;

    # Disable buffering for streaming responses
    proxy_buffering off;
    proxy_request_buffering off;

    # Allow large request bodies (for model file uploads if needed)
    client_max_body_size 10G;
}

# Optional: Health check endpoint
location /ollama-health {
    proxy_pass http://127.0.0.1:11434/api/tags;
    proxy_set_header Host $host;
}
